# Tutorial 1

Paper: Embodied Intelligence via Learning and Evolution

Inductive biases

RNN = MLP where layer depth is replaced with time. 

The weight vectors are normal to the decision boundary.

Universal function approximation theorem

Deep expressivite

How to approximate the cosine with the ReLU?

2D probability simplex in 3D space.

Overfitting the test dataset on the hyperparameters

R^2 = fraction of explained variance.

# Tutorial 2

Exponential expressivity in deep neural networks through transient chaos, NeuroIPS 2016.

Untangling invariant object recognition, Trends in Cognitive Sciences, 2007

Expressivity is not enough: we need learnability also.

Ba & Caruana (2014), relates to knowledge destilation.

The polynomials are the new features.

Credit assignment: related to loacal learning rules to weight changes

Maybe poisson spiking achieves maintaining an spontaneous level of activity in the edge of chaos.

