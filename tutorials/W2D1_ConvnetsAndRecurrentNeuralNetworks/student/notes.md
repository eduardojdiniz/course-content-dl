# Tutorial 1 -  Introduction to CNNs

Alona Fyshe

Regularization reduces the **effective** number of parameters in a model.

CNN: Parameter share (aka weight-sharing) over space
RNN: Parameter share over time

Pooling == Subsampling. Insensitive to small local changes (local translation invariance). It reduces the dimensions of the data, without additional parameters.

Krizhevsky et al (2012)

Horikawa & Kamitani (2017)

A representation is a construct that approximates the entities and/or relations in the real object.

Is there a better way to do padding? Maybe try to extend the image, i.e. predict the edge pixels?

Is there a better way to determine strides?

Filters = global translation invariance.

The ReLU kinda removes (threshold) information outside the kernel.

# Tutorial 2 - Introduction to RNNs

karpathy.github.io "mn-effectiveness"