W1D1 - Session 2.5
Expectation = 95% DL, 5% setup
Reality = 95% setup, 5% DL

W1D2 - Tutorial 2, Interactive Demo 2.1 
I must have lots of layers in my brain, that's why I learn things so slowly.

Smart people optimise all hyperparameter for their brain architecture? Or they figure out how to properly initialize their synaptic weights? Or less number of layers?

Glass of milk

W1D3 - Tutorial 3

Sharks don't have bones?!!

we explicitly forbid the use of primate brain responses to the test video set. 

The brain implements Adam and Eve

karpathy's constant: "3e-4 is the best learning rate for Adam, hands down."

If the network goes too deep, it ends up with an existential crisis.

I thought you end up in Limbo if you go too deep.

MidLayer crisis

When getting money, you tell you doing DL, when hiring tell you doing ML, when implementing, do Multilinear Regression.

https://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/

It is very hard to tell a random florest the world is translation invariant

AI on the streets, Linear regression in the sheets

For drawing pod mascots: https://colab.research.google.com/drive/1IHzJRVxCGtNY-UhqlD-yFqNlwd3SbHd7?usp=sharing

Sometimes I think it would be faster to train a monkey to do the task than to clean-up the data.

Pass the torch


Leonardo DiCaprio soon will release a documnetary how the cows are not the killers of the envirorment, but Transformers are. Transformeless. A new type of diet called transformeless.