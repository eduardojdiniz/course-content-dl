W1D1 - Session 2.5
Expectation = 95% DL, 5% setup
Reality = 95% setup, 5% DL

W1D2 - Tutorial 2, Interactive Demo 2.1 
I must have lots of layers in my brain, that's why I learn things so slowly.

Smart people optimise all hyperparameter for their brain architecture? Or they figure out how to properly initialize their synaptic weights? Or less number of layers?

Glass of milk

W1D3 - Tutorial 3

Sharks don't have bones?!!

we explicitly forbid the use of primate brain responses to the test video set. 

The brain implements Adam and Eve

karpathy's constant: "3e-4 is the best learning rate for Adam, hands down."

If the network goes too deep, it ends up with an existential crisis.

I thought you end up in Limbo if you go too deep.

MidLayer crisis

When getting money, you tell you doing DL, when hiring tell you doing ML, when implementing, do Multilinear Regression.

https://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/

It is very hard to tell a random florest the world is translation invariant

AI on the streets, Linear regression in the sheets

For drawing pod mascots: https://colab.research.google.com/drive/1IHzJRVxCGtNY-UhqlD-yFqNlwd3SbHd7?usp=sharing

Sometimes I think it would be faster to train a monkey to do the task than to clean-up the data.

Pass the torch


Leonardo DiCaprio soon will release a documnetary how the cows are not the killers of the envirorment, but Transformers are. Transformeless. A new type of diet called transformeless. Use the "we need to go depper" leornard dicaprio meme.

Those lectures from the LSTM day are just a LSTM cell, it forget frames, add frames, concatenate frames...a lot of forget gates for me today

W2D4:
Here is the answer from an actual neural network to the question of what is better, shallow or deep neural networks 

Q: The best neural network is shallow/deep

A: P(shallow) is +inf times higher than P(deep) 

They already learned irony

we need a non-binary gender transformer 

Think! 7.2: Biases of using these models in other fields
Recently people started to apply language models outside of natural languages. For instance, ProtBERT is trained on the sequences of proteins. Think about the types of bias that might arise in this case.

On the panel they said they care about interpretabiliy in drug discovery, but if you cannot interpret your models, how can assess if the model is not biased?

If you only study on male mice models, aren't we biasing the drugs towards man?

If we synthesize a drug for Alzheimer's disease and our databank has more data about a specific type of Alzheimer's, we are biasing our drugs.

I will be biased towards transformers over RNNs after NMA since I got nothing from the RNN day yesterday 

combine an RNN with a transformer to get a transformed RNN 

we should train a Transformer to fill out those airtables 


You want to finish training your model, subscribe now to google.colab.pro