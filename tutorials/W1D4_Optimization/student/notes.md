# Tutorial 1: Optimization techniques

Picking the loss function
**(AUC) Area Under Curve**: for class imbalance, instead of accuracy.

Poor conditionining: presence of both low and high curvature --> Slow convergence.

With momentum we see an overshoot past the solution, and therefore oscillations.

Overparametrization can reduce the degree of non-convexity of a function and therefore disensitize the effects of initial condition.

Increasing the learning rate can overfit to the mini-batches or the fact that we are remembering bad updates.

Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks.

[Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization](https://epubs.siam.org/doi/abs/10.1137/070697835?casa_token=unU18fqxSt8AAAAA%3AQ4TMEJzgxTMx1OhU-T9EyEzZ_o7wWtlY5TRVyK1BdpWJURWYR_tRQsq4advpomqZRfd04SCnKsW4&journalCode)

Double, triple, quadruptible, ..., descent.

[Low rank saddle free newton scalable stochastic nonconvex optimization](https://arxiv.org/pdf/2002.02881.pdf)

[Nesterov momentum](https://dominikschmidt.xyz/nesterov-momentum/)

[Noncausal learning in Learning to learn](https://jotterbach.github.io/content/posts/causal_noncausal_learning/2015-12-13-Causal_vs_Noncausal_Learning/)

intuition: wide minima means exact parameter settings dont matter -> uses less information -> generalizes better

Get a good understand of the data and then teach that intuition to the model.

[A critique of pure learning and what artificial neural networks can learn from animal brains](https://www.nature.com/articles/s41467-019-11786-6)

[High-dimensional geometry of population responses in visual cortex
](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6642054/)

[Konrad's on podcast](Neurotech Pub Episode 4: Trading Spaces: Dimensionality Reduction for Neural Recordings)

[Estimating the intrinsic dimension of datasets by a minimal neighborhood information](https://www.nature.com/articles/s41598-017-11873-y)

[Visualization for AI Explainability](https://visxai.io/)

[Object Based Attention Through Internal Gating](https://arxiv.org/pdf/2106.04540.pdf)

[Ludwig, a code-free deep learning toolbox](https://eng.uber.com/introducing-ludwig/)

[Benign overfitting in linear regression](https://www.pnas.org/content/117/48/30063#sec-12)